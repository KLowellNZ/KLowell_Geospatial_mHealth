{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Winners of European Soccer Matches\n",
    "\n",
    "The goal of this assignment is to get some practice working with a somewhat messy relational database and building a multiclass classification model for complex data. Download the soccer database [here](https://www.kaggle.com/hugomathien/soccer) if you haven't already. Be sure to refer to the documentation on the website for information on what the variables in the dataset represent.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The 'deliverable' at the end of this assignment is a predictive model. The minimum requirements are simple:\n",
    "\n",
    "1. **Clean/Preprocess the data, Select Features:** Clean and preprocess the data, then select features for inclusion. Your model must incorporate information from **at least one** table in addition to the 'match' table. \n",
    "\n",
    "1. **Split the data:** Split the processed dataset into a training set and a test set. The size of the test set should be no more than 20% of the size of the full dataset. Make sure that the test set consists of randomly selected rows from the dataset, not just the first or last 20% of the data. You can use a function from scikit-learn for this.\n",
    "\n",
    "2. **Construct a model:** Using the training data ONLY, build the best model you can for predicting whether a given match is a home team win, an away team win, or a draw. Validate your model initially using $k$-fold cross-validation on the training set. Then, when you are satisfied with your results on the training set, make predictions on the test set and consider the precision and recall scores of your model. You can use sklearn's classification report to report these values.\n",
    "\n",
    "3. **Discuss your results:** How did your model perform? Which features were most informative? Which were not? What might have helped to improve the quality of your results? \n",
    "\n",
    "Put all of the work that you would like me to see in this notebook. Use Markdown cells to explain the steps in your analysis. Include visualizations as appropriate. When you've completed the assignment, export a copy of the notebook to html by clicking File > Download As > HTML. Submit the html file only on myCourses by the due date.\n",
    "\n",
    "\n",
    "\n",
    "## Submission & Due Date\n",
    "\n",
    "Submit on myCourses by Tuesday, August 1, 11:59PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS FILE USES BETTING ODDS FROM THREE BETTING HOUSES AND ALL TWO WAY INTERACTIONS AMONG THEM. IT DOES NOT USE ANY INFORMATION FROM EACH TEAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "# create database connection\n",
    "conn = sqlite3.connect('C:/Analytics/DATA803/PythonII/PythonII_Assignments/soccer/database.sqlite')\n",
    "\n",
    "# read the sql query and return a Pandas dataframe\n",
    "# This reads meta-data from the SQLite database.  Each of these pieces\n",
    "# of information are in separate tables.  We need to get the tables\n",
    "#\n",
    "pd.read_sql_query('select * from sqlite_master where type=\"table\"', conn)\n",
    "\n",
    "# This brings in 3 tables as 3 dataframes.\n",
    "match = pd.read_sql_query('select * from Match', conn)\n",
    "team_attributes = pd.read_sql_query('select * from Team_Attributes', conn)\n",
    "team = pd.read_sql_query('select * from Team', conn)\n",
    "\n",
    "# For each team grab random set of attributes regardless of dates.  \n",
    "# Then shrink data frame.  SO NOW TEAM_ATTS CONTAINS OUR TEAM \n",
    "# ATTRIBUTES RATHER THAN THE DF TEAM_ATTRIBUTES\n",
    "teams_w_atts = team_attributes['team_api_id'].unique()\n",
    "\n",
    "unique_team_att_idx = [] #Create index for values selected at random.\n",
    "for tm in teams_w_atts:\n",
    "    idx = team_attributes[team_attributes['team_api_id'] == tm].index\n",
    "    unique_team_att_idx.append(np.random.choice(idx, size=1)[0])\n",
    "    \n",
    "team_atts = team_attributes.loc[unique_team_att_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are functions we need.\n",
    "def change_column_name(dataframe=None, current=None, to=None):\n",
    "    cols = list(dataframe.columns)\n",
    "    for i, col in enumerate(cols):\n",
    "        if col == current:\n",
    "            cols[i] = to\n",
    "        else:\n",
    "            continue\n",
    "    return cols\n",
    "# This function will attach the \"add\" word to all variables to\n",
    "# distinguish home from away variables that will be merged.\n",
    "def change_all_column_names(dataframe=None,add=None):\n",
    "    cols=list(dataframe.columns)\n",
    "    for i,col in enumerate(cols):\n",
    "        cols[i]=add+col\n",
    "    return cols\n",
    "# Now assign this to the team_attribute variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This box retains only the team_atts variables that what we want\n",
    "# and calculates the result of each match. Determine if a match\n",
    "#was a draw(1), home win(2), an away win(3).\n",
    "#match.drop(['id'], axis=1, inplace=True)\n",
    "match['matchresult']=1\n",
    "match['matchresult'] = np.where((['home_team_goal'] > \\\n",
    "            match['away_team_goal']),2,match['matchresult'])\n",
    "match['matchresult'] = np.where((match['home_team_goal'] < \\\n",
    "            match['away_team_goal']),3,match['matchresult'])\n",
    "# Name variables to keep.\n",
    "match =match[['matchresult','home_team_api_id', 'away_team_api_id',\\\n",
    "          'B365H', 'B365D', 'B365A','IWH', 'IWD', 'IWA', \\\n",
    "             'BWH', 'BWD', 'BWA']]\n",
    "team_atts.drop(['id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Also get rid of all player variables -- i.e., retain all variables\n",
    "# that do not have 'play' in position 5:8.\n",
    "#cols=[c for c in match.columns if c.lower()[5:9]!= 'play']\n",
    "#match=match[cols]\n",
    "# might as well datetime these\n",
    "#match['date'] = pd.to_datetime(match['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell merges data from the team_atts and match databases.\n",
    "# This calls the function change column names so we have the \n",
    "# same variable in both databases.  Then we merge MATCH with\n",
    "# TEAM_ATTS on the field team_api_id.  Do Home first and then away.\n",
    "# Create a list of variable names for TEAM_ATTS subsequent use.\n",
    "origcols=team_atts.columns\n",
    "cols = change_column_name(dataframe=match, current='home_team_api_id', \\\n",
    "            to='team_api_id')\n",
    "match.columns = cols\n",
    "# Add \"home\" to all of the team_atts attributes.\n",
    "cols=change_all_column_names(dataframe=team_atts,add='home_')\n",
    "team_atts.columns=cols\n",
    "# 'home_' has been added to to the join field team_api_id. Get\n",
    "# rid of it before the merge for the merge field.\n",
    "cols = change_column_name(dataframe=team_atts, \\\n",
    "            current= 'home_team_api_id',to='team_api_id')\n",
    "team_atts.columns = cols\n",
    "# Now merge the match and team_attributes. Start with a clean\n",
    "# dataframe.\n",
    "match_w_home_atts = pd.DataFrame()\n",
    "match_w_home_atts = match.merge(team_atts, on='team_api_id', how='left')\n",
    "# Now change team_api_id back to home_team_api_id in match_w_home_atts\n",
    "# and in match.\n",
    "cols = change_column_name(dataframe=match_w_home_atts, current= \\\n",
    "        'team_api_id', to='home_team_api_id')\n",
    "match_w_home_atts.columns = cols\n",
    "cols = change_column_name(dataframe=match, current= \\\n",
    "        'team_api_id', to='home_team_api_id')\n",
    "match.columns = cols\n",
    "# Now restore the original names of TEAM_ATTS so HOME does not\n",
    "# show up in the away attributes.\n",
    "team_atts.columns = origcols\n",
    "\n",
    "# Now do away team.\n",
    "cols = change_column_name(dataframe=match_w_home_atts, current=\\\n",
    "            'away_team_api_id', to='team_api_id')\n",
    "match_w_home_atts.columns = cols\n",
    "# Add \"away\" to all of the team_atts attributes.\n",
    "cols=change_all_column_names(dataframe=team_atts,add='away_')\n",
    "team_atts.columns=cols\n",
    "# 'away_' has been added to to the join field team_api_id. Get\n",
    "# rid of it before the merge.\n",
    "cols = change_column_name(dataframe=team_atts, current='away_team_api_id', \\\n",
    "            to='team_api_id')\n",
    "team_atts.columns = cols\n",
    "# Now merge the match and team_attributes.\n",
    "match_w_homeaway_atts = match_w_home_atts.merge(team_atts, \\\n",
    "            on='team_api_id', how='left')\n",
    "# Now change team_api_id back to away_team_api_id in new file and\n",
    "# match.\n",
    "cols = change_column_name(dataframe=match_w_homeaway_atts, current= \\\n",
    "        'team_api_id', to='away_team_api_id')\n",
    "match_w_homeaway_atts.columns = cols\n",
    "cols = change_column_name(dataframe=match_w_homeaway_atts, current= \\\n",
    "        'team_api_id', to='away_team_api_id')\n",
    "# Now change team_atts back to the original and get rid of team_ids\n",
    "# from match_w_homeaway_atts.\n",
    "team_atts.columns = origcols\n",
    "# Now get rid of home and away team identifiers and keep only betting\n",
    "# house information to prepare the data for modelling.\n",
    "match_w_homeaway_atts=match_w_homeaway_atts[['matchresult',\\\n",
    "            'B365H', 'B365D', 'B365A','IWH', 'IWD', 'IWA', \\\n",
    "             'BWH', 'BWD', 'BWA']]\n",
    "#match_w_homeaway_atts.drop(['home_team_api_id','away_team_api_id', \\\n",
    "#            'home_team_fifa_api_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22502, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_w_homeaway_atts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22502, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell eliminats missing values. It starts by eliminating\n",
    "# columns that have more than THRESHOLD percentage of missing values.\n",
    "# From the resulting dastaaframe, iIt then eliminates all rows\n",
    "# having any missing values.\n",
    "threshold=0.15\n",
    "maxcols=match_w_homeaway_atts.shape[1]-1\n",
    "for i in range(maxcols,0,-1):\n",
    "    nummiss = match_w_homeaway_atts[match_w_homeaway_atts.columns[i]].isnull().sum()\n",
    "    total=match_w_homeaway_atts.shape[0]\n",
    "    if nummiss/total < threshold:\n",
    "        continue\n",
    "    print('Dropped: ',i,match_w_homeaway_atts.columns[i],nummiss,total,nummiss/total)\n",
    "    match_w_homeaway_atts.drop(match_w_homeaway_atts.columns[i],axis=1,inplace=True)\n",
    "# Now eliminate all rows having missing values.\n",
    "match_w_homeaway_atts=match_w_homeaway_atts.dropna(axis=0)\n",
    "match_w_homeaway_atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell is for getting interactions and transformations.\n",
    "#Now loop through the data frame for variables 2 [index=1] through\n",
    "# n. In full implementation, the y variable will be in its own df\n",
    "# that will get converted to an array for fitting the logistic\n",
    "# regression. If a character variable is found, convert it to dummy\n",
    "# variables.If a number is found, make it floating point. First set up\n",
    "# df.\n",
    "# NOTE: matchresult is the first column and it is never part of the\n",
    "# interactions.  Start all loops at index 1 and not 0.\n",
    "soccer=match_w_homeaway_atts\n",
    "outsoccer=pd.DataFrame()\n",
    "outsoccer=match_w_homeaway_atts[['matchresult']]\n",
    "for i in range(1,soccer.shape[1]):\n",
    "# Create an empty dataframe for filling with variable(s).\n",
    "    newcol=pd.DataFrame()\n",
    "#Test if this is a numeric variable. If so, make sure it is type float\n",
    "# in the output file.        \n",
    "    if(np.issubdtype(soccer[soccer.columns[i]], int)) or \\\n",
    "      (np.issubdtype(soccer[soccer.columns[i]], float)):\n",
    "        newcol[0]=soccer[soccer.columns[i]].astype(float)\n",
    "        col=soccer.columns[i]\n",
    "        newcol.columns=[col]\n",
    "        outsoccer=pd.concat([outsoccer,newcol],axis=1)\n",
    "########### THIS IS THE NO TRANSFORM FILE.  DO NOT USE SQUARED OR\n",
    "########### INVERSE TRANSFORMATIONS.\n",
    "# Add a squared transformation.\n",
    "#        newcol=pd.DataFrame()\n",
    "#        newcol[0]=(soccer[soccer.columns[i]].astype(float))**2\n",
    "#        col='sqrd'+soccer.columns[i]\n",
    "#        newcol.columns=[col]\n",
    "#        outsoccer=pd.concat([outsoccer,newcol],axis=1)\n",
    "# Now add an inverse transformation. Add one before taking inverse\n",
    "# to protect against zero division without substantially impacting\n",
    "# model results.\n",
    "#        newcol=pd.DataFrame()\n",
    "#        newcol[0]=1/((soccer[soccer.columns[i]].astype(float)+1))\n",
    "#        col='inv'+soccer.columns[i]\n",
    "#        newcol.columns=[col]\n",
    "#        outsoccer=pd.concat([outsoccer,newcol],axis=1)\n",
    "############# INCLUDE THE CODE ABOVE IF WE WANT TRANSFORMS\n",
    "# If it is not a numeric variable, make dummy variables and\n",
    "# append them to the output file.\n",
    "    else:\n",
    "        newcol=pd.get_dummies(soccer[soccer.columns[i]],drop_first=True)\n",
    "        cols=change_all_column_names(dataframe=newcol,add=\\\n",
    "                soccer.columns[i])\n",
    "        newcol.columns=cols\n",
    "        outsoccer=pd.concat([outsoccer,newcol],axis=1)\n",
    "# Now that we have all base variables, calculate all two-way\n",
    "# interactions and append them to the file.  Note all variables\n",
    "# in the file are now numeric. Because I will concatentate to the\n",
    "# outsoccer file, the number of columns will increase. Create a\n",
    "# variable for looping that will control the number of columns addressed.\n",
    "maxcols=outsoccer.shape[1]\n",
    "for i in range(1,maxcols-1):\n",
    "    for j in range(i+1,maxcols):\n",
    "# Create empty dataframe for each interaction.\n",
    "        newcol=pd.DataFrame()\n",
    "# Multiply each column by all columns -- but duplication and squaring\n",
    "# is avoided through the indexing.\n",
    "        newcol[0]=outsoccer[outsoccer.columns[i]]* \\\n",
    "            outsoccer[outsoccer.columns[j]]\n",
    "        col=outsoccer.columns[i]+outsoccer.columns[j]\n",
    "        newcol.columns=[col]\n",
    "# Do not write columns full of zeroes resulting from the \n",
    "# multiplication of dummy variables from the same original\n",
    "# categorical variable. Also do not write a variable that\n",
    "# is the multiplication of a variable squared by itself \n",
    "# (thereby making a cubed variable which we do not want),\n",
    "# of a variable that is a variable multiplied by its inverse.\n",
    "        if(max(newcol.ix[:,0])> 0) and \\\n",
    "            sum(newcol.ix[:,0]) != sum(outsoccer.ix[:,i]**3):\n",
    "            outsoccer=pd.concat([outsoccer,newcol],axis=1)\n",
    "# In case calculations caused problems, drop all rows with \n",
    "# missing values.\n",
    "outsoccer=outsoccer.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22502, 46)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsoccer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep the arrays to fit the data.\n",
    "y=np.array(outsoccer['matchresult'])\n",
    "y\n",
    "\n",
    "outsoccer.drop(['matchresult'],axis=1,inplace=True)\n",
    "X=outsoccer.loc[:,:].as_matrix()\n",
    "\n",
    "# Now split the data into training and test.\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.45      0.00      0.00      5686\n",
      "          2       0.54      0.87      0.67     10340\n",
      "          3       0.51      0.47      0.49      6476\n",
      "\n",
      "avg / total       0.51      0.53      0.45     22502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now fit the multinomial regression model (lbfgs.)\n",
    "#lmsk = LogisticRegression(solver='lbfgs', \\\n",
    "#        multi_class='multinomial').fit(Xtr, ytr)\n",
    "lmsk = LogisticRegressionCV(solver='lbfgs',cv=8, \\\n",
    "        multi_class='multinomial').fit(Xtr, ytr)\n",
    "preds = lmsk.predict(X)\n",
    "#lm.coef_\n",
    "print(classification_report(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1    2\n",
      "0  1   859  298\n",
      "1  0  1774  269\n",
      "2  1   675  624\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix for the sklearn model and test data.\n",
    "predslmsk=lmsk.predict(Xte)\n",
    "ppredslmsk=lmsk.predict_proba(Xte)\n",
    "cnfmtrx=pd.DataFrame(confusion_matrix(yte,predslmsk))\n",
    "print(cnfmtrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532992668296 0.673756171667 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get the overall accuracy, precision, and recall (for test data).\n",
    "acc=accuracy_score(yte, predslmsk)\n",
    "prec=cnfmtrx.iat[1,1]/(cnfmtrx.iat[1,1]+cnfmtrx.iat[0,1])\n",
    "rec=cnfmtrx.iat[1,1]/(cnfmtrx.iat[1,1]+cnfmtrx.iat[1,0])\n",
    "print(acc, prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
